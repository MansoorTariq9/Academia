{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 21\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 24\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 19\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 8\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 7\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 24\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 7\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 7\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 9\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 4\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 8\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 29\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 5\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 5\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 5\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 10\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 10\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 10\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 5: expected 1 fields, saw 2\\n'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "#from snowballstemmer import stemmer\n",
    "\n",
    "def urdu_stemming(word):\n",
    "  word2 = copy.deepcopy(word)\n",
    "  suffixes = [\"یں\", \"وں\", \"ے\", \"یاں\"]\n",
    "  for suffix in suffixes:\n",
    "    if word2.endswith(suffix):\n",
    "      word2 = word2[:-len(suffix)]\n",
    "  return word\n",
    "\n",
    "def remove_non_urdu(text):\n",
    "    urdu_alphabets = u'\\u0600-\\u06FF'\n",
    "    pattern = f'[^{urdu_alphabets}]'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[,.;:]', '', text)\n",
    "    return text\n",
    "\n",
    "stop_words = set(pd.read_csv(\"Urdu stopwords.txt\", squeeze=True).values)\n",
    "\n",
    "global file_count\n",
    "file_count = 0\n",
    "\n",
    "global term_id\n",
    "term_id = 0\n",
    "\n",
    "global term_id1\n",
    "term_id1 = {}\n",
    "\n",
    "global forward_index\n",
    "forward_index = {}\n",
    "\n",
    "def process_dir(dir_name):\n",
    "    global file_count\n",
    "    global term_id\n",
    "\n",
    "    for file in os.listdir(dir_name):\n",
    "        file_count += 1\n",
    "        file_name = f\"{file_count}/{file}\\n\"\n",
    "        with open(\"docids.txt\", \"a\") as f:\n",
    "            f.write(file_name)\n",
    "\n",
    "        df = pd.read_csv(dir_name + file , error_bad_lines=False)\n",
    "        data = remove_non_urdu(df.to_string())\n",
    "\n",
    "        tokens = data.split()\n",
    "        tokens = [urdu_stemming(i) for i in tokens if i not in stop_words]\n",
    "\n",
    "        for word in tokens:\n",
    "            if word not in term_id1:\n",
    "                term_id1[word] = term_id\n",
    "                term_id += 1 \n",
    "\n",
    "        with open(os.path.join(dir_name, file), 'r') as file1:\n",
    "            text = file1.read() \n",
    "\n",
    "        pos_dict = {term_id1[word]:[i for i, x in enumerate(tokens) if x == word] for word in text.split() if word in term_id1}\n",
    "\n",
    "        tokens1=text.split()\n",
    "        \n",
    "        indices=[]\n",
    "        for word in set(tokens1):\n",
    "                indices = [i for i in range(len(tokens1)) if tokens1 [i] == word]\n",
    "                #indices = [i+1 for i in range(len(tokens1)) if tokens1[i] == word]\n",
    "                # for i in range(len(tokens1)):\n",
    "                #     if tokens1[i] == word:\n",
    "                #         print(\"meow : \",tokens1[i],word)\n",
    "                if word in term_id1:\n",
    "                    pos_dict[term_id1[word]]=indices\n",
    "        forward_index[file] = pos_dict\n",
    "        \n",
    "        #store the forward index in a txt file in append mode\n",
    "        with open(\"doc_index.txt\", \"a\") as f:\n",
    "            #first write the file name SPACE term_id SPACE position\n",
    "            for key, value in forward_index[file].items():\n",
    "                f.write(str(file_count) + \" \" + str(key) + \" \")\n",
    "                for i in value:\n",
    "                    f.write(str(i) + \" \")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "     \n",
    "    for key, value in term_id1.items():\n",
    "        #store key and value in a txt file in append mode\n",
    "        with open(\"termids.txt\", \"a\") as f:\n",
    "            f.write(str(key) + \"/\" + str(value) + \"\\n\")\n",
    "\n",
    "path=\"C:/Users/HP/Downloads/Documents (2)/Documents/\"\n",
    "\n",
    "process_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global inverted_index\n",
    "inverted_index = {}\n",
    "\n",
    "#read the forward_index.txt file and store it in a dictionary\n",
    "with open(\"doc_index.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        file = line[0]\n",
    "        term_id = int(line[1])\n",
    "        positions = [int(x) for x in line[2:]]\n",
    "        if term_id not in inverted_index:\n",
    "            inverted_index[term_id] = {}\n",
    "        inverted_index[term_id][file] = positions\n",
    "\n",
    "\n",
    "previous_key=0\n",
    "#store the inverted index in a txt file in append mode\n",
    "with open(\"term_index.txt\", \"a\") as f:\n",
    "    #first store term_id\n",
    "    for key, value in inverted_index.items():\n",
    "        f.write(str(key) + \" \")\n",
    "        #then store the file name and the position\n",
    "        previous_key=0\n",
    "        for key1, value1 in value.items():\n",
    "            f.write(str(int(key1)-int(previous_key)) + \":\")\n",
    "            previous_key=key1\n",
    "            previous_i=0\n",
    "            for i in value1:\n",
    "                f.write(str(i-previous_i) + \" \")\n",
    "                previous_i=i\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "doc1=0\n",
    "temp={}\n",
    "doc1={}\n",
    "pos1={}\n",
    "#read the inverted_index.txt file and store the number of occurence of each term in a dictionary temp and number of doucments in a variable\n",
    "with open(\"term_index.txt\", \"r\") as f:\n",
    "    length=0\n",
    "\n",
    "    for line in f:\n",
    "        a = line.split()\n",
    "        #if there is a : in the line then increment the number of documents\n",
    "        term_id = int(a[0])\n",
    "        pos1[term_id]=length\n",
    "\n",
    "\n",
    "        temp[term_id]=len(a)-1\n",
    "        \n",
    "        b=line.split(\":\")\n",
    "        doc1[term_id]=len(b)-1 \n",
    "\n",
    "        length=len(line)+length\n",
    "        \n",
    "\n",
    "#store the number of occurence of each term in a txt file in append mode\n",
    "with open(\"term_info.txt\", \"a\") as f:\n",
    "    for key, value in temp.items():\n",
    "        f.write(str(key) + \"/\" + str(pos1[key]) +\"/\" + str(value)+\"/\"+ str(doc1[key]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-106b3ffed1b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_info' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
